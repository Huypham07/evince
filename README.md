# evince

<p align="center">
  <b>Evidence-Verified INtegrity Checker for ESG Claims</b><br>
  <i>A Novel Framework for ESG-Washing Detection in Vietnamese Banking Reports</i>
</p>

---

## ğŸ¯ Overview

**evince** lÃ  má»™t framework Deep Learning Ä‘á»ƒ phÃ¡t hiá»‡n **ESG-Washing** (táº©y xanh) trong bÃ¡o cÃ¡o thÆ°á»ng niÃªn cá»§a cÃ¡c ngÃ¢n hÃ ng Viá»‡t Nam. Framework sá»­ dá»¥ng **PhoBERT** lÃ m encoder ngÃ´n ngá»¯ vÃ  tÃ­ch há»£p **Claim-Evidence Linking** Ä‘á»ƒ phÃ¢n tÃ­ch á»Ÿ má»©c Ä‘á»™ document.

### TÃ­nh nÄƒng chÃ­nh

| Feature | MÃ´ táº£ |
|---------|-------|
| ğŸ·ï¸ **ESG Classification** | PhÃ¢n loáº¡i cÃ¢u vÄƒn vÃ o 6 chá»§ Ä‘á» ESG (max 512 tokens) |
| ğŸ” **Washing Detection** | PhÃ¡t hiá»‡n 7 loáº¡i ESG-Washing vá»›i attention explainability |
| ğŸ“„ **Document Analysis** | PhÃ¢n tÃ­ch má»©c Ä‘á»™ washing toÃ n bá»™ document |
| ğŸ”— **Claim-Evidence Linking** | LiÃªn káº¿t cam káº¿t vá»›i báº±ng chá»©ng há»— trá»£ |
| ğŸ“ **Semantic Chunking** | Xá»­ lÃ½ raw OCR thÃ nh semantic chunks vá»›i token limit |
| ğŸ¤– **LLM Labeling** | Táº¡o nhÃ£n tá»± Ä‘á»™ng vá»›i Gemini, Bedrock, hoáº·c Qwen3 |
| ğŸ‹ï¸ **Training Pipeline** | Train custom models vá»›i labeled data |

---

## ğŸ—ï¸ Project Structure

```
evince/
â”œâ”€â”€ main.py                 # ğŸš€ CLI entry point
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ requirements.txt        # Dependencies
â”‚
â”œâ”€â”€ data/                   # ğŸ“Š Data directory
â”‚   â”œâ”€â”€ raw_ocr_annual_report.zip  # Raw OCR text files
â”‚   â”œâ”€â”€ all_chunks.csv             # Processed chunks
â”‚   â””â”€â”€ labeled.csv                # LLM-labeled data
â”‚
â”œâ”€â”€ core/                   # ğŸ”§ Core utilities
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ gemini_client.py    # Google Gemini LLM
â”‚   â”œâ”€â”€ bedrock_client.py   # AWS Bedrock LLM
â”‚   â””â”€â”€ qwen_client.py      # Qwen3 LLM
â”‚
â”œâ”€â”€ models/                 # ğŸ§  Classification models
â”‚   â”œâ”€â”€ esg_topic_classifier.py    # ESG 6-class classifier (512 tokens)
â”‚   â””â”€â”€ washing_detector.py        # Washing 7-class detector
â”œâ”€â”€ claim_evidence/         # ğŸ”— Claim-Evidence Linking
â”œâ”€â”€ training/               # ğŸ‹ï¸ Training pipeline
â”‚   â”œâ”€â”€ train.py            # Trainer class
â”‚   â””â”€â”€ data_loader.py      # Dataset & DataLoader (512 tokens)
â”œâ”€â”€ evaluation/             # ğŸ“ˆ Metrics
â””â”€â”€ scripts/                # ğŸ“œ Utility scripts
    â”œâ”€â”€ llm_labeling.py     # LLM-based pseudo-labeling (multi-threaded)
    â””â”€â”€ process_ocr_semantic.py  # Smart OCR processing
```

---

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Clone repo
git clone https://github.com/Huypham07/evince.git
cd evince

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Setup env
cp .env.example .env
# Edit .env vá»›i API keys cá»§a báº¡n
```

### 2. Process Raw OCR â†’ Semantic Chunks

Xá»­ lÃ½ raw OCR files thÃ nh semantic chunks vá»›i token limit:

```bash
# Xá»­ lÃ½ file Ä‘Æ¡n
python main.py process --input data/bctn_2024_raw.txt --output data/chunks.csv

# Xá»­ lÃ½ zip chá»©a nhiá»u file (vÃ­ dá»¥: 11 banks Ã— 5 years)
python main.py process --input data/raw_ocr_annual_report.zip --output data/all_chunks.csv
```

**Output:**
```
ğŸ“Š Statistics:
  Total chunks: 30604
  Paragraph chunks: 18266
  Table chunks: 12338
  Average token count: 160
  Banks: ['vib', 'viettinbank', 'mbbank', 'shb', 'bsc', 'vietcombank', ...]
  Years: [2015, 2017, 2018, 2020, 2021, 2022, 2023, 2024]
```

### 3. Generate Labels with LLM

Táº¡o training labels vá»›i LLM (Gemini/Bedrock/Qwen):

```bash
# Cáº¥u hÃ¬nh trong .env:
# LLM_PROVIDER=gemini
# GOOGLE_API_KEY=your_api_key

# Label toÃ n bá»™ dataset (multi-threaded)
python main.py label -i data/all_chunks.csv -o data/labeled.csv --workers 4

# Hoáº·c sample nhá» Ä‘á»ƒ test
python main.py label -i data/all_chunks.csv -o data/labeled.csv --sample 100
```

**Output:**
```
=== Label Distribution ===

ESG Topics:
  Non-ESG: 18836 (61.7%)
  G: 7588 (24.8%)
  S: 2726 (8.9%)
  Financing: 688 (2.3%)
  E: 399 (1.3%)
  Policy: 310 (1.0%)

Washing Types:
  NOT_WASHING: 26452 (86.6%)
  VAGUE_COMMITMENT: 2383 (7.8%)
  SYMBOLIC_ACTION: 787 (2.6%)
  FUTURE_DEFLECTION: 530 (1.7%)
```

### 4. Train Custom Models ğŸ‹ï¸

**Train ESG Topic Classifier:**
```bash
python main.py train \
    --model-type esg \
    --input data/labeled.csv \
    --epochs 5 \
    --batch-size 16 \
    --output-dir ./checkpoints/esg
```

**Train Washing Detector:**
```bash
python main.py train \
    --model-type washing \
    --input data/labeled.csv \
    --epochs 10 \
    --batch-size 16 \
    --output-dir ./checkpoints/washing
```

**Training Options:**
| Option | Default | Description |
|--------|---------|-------------|
| `--model-type` | required | `esg` or `washing` |
| `--epochs` | 5 | Number of epochs |
| `--batch-size` | 16 | Batch size |
| `--learning-rate` | 2e-5 | Learning rate |
| `--max-length` | 512 | Max token length |
| `--val-split` | 0.1 | Validation split |
| `--freeze-layers` | 0 | BERT layers to freeze |
| `--device` | auto | cpu/cuda/auto |

### 5. Classify ESG Topics

```bash
# Classify tá»« file
python main.py classify --input data/chunks.csv --output data/classified.csv

# Classify single text
python main.py classify --text "NgÃ¢n hÃ ng cam káº¿t giáº£m 30% phÃ¡t tháº£i carbon vÃ o nÄƒm 2030"
```

### 6. Document Analysis (Washing Detection) ğŸ”

```bash
python main.py analyze --input data/classified.csv --bank agribank --year 2024
```

**Output:**
```
============================================================
DOCUMENT ANALYSIS RESULT
============================================================
Bank: agribank | Year: 2024
Document Washing Index: 0.412
High Risk Claims: 5

âš ï¸  HIGH RISK CLAIMS DETECTED:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[1] Claim: "NgÃ¢n hÃ ng cam káº¿t Ä‘áº¡t Net Zero vÃ o nÄƒm 2050"
    Risk Level: HIGH
    Verification Score: 0.120
    Evidence Found: (No relevant evidence found)
```

---

## ğŸ“Š Label Definitions

### ESG Topic Classification (6 classes)

| Label | Code | Description |
|-------|------|-------------|
| Environmental | `E` | MÃ´i trÆ°á»ng, khÃ­ háº­u, nÄƒng lÆ°á»£ng, carbon |
| Social | `S` | NhÃ¢n viÃªn, cá»™ng Ä‘á»“ng, sá»©c khá»e, Ä‘Ã o táº¡o |
| Governance | `G` | Quáº£n trá»‹, Ä‘áº¡o Ä‘á»©c, tuÃ¢n thá»§, rá»§i ro |
| ESG Financing | `Financing` | TÃ­n dá»¥ng xanh, trÃ¡i phiáº¿u ESG |
| Policy | `Policy` | Chiáº¿n lÆ°á»£c, chÃ­nh sÃ¡ch ESG |
| Non-ESG | `Non-ESG` | KhÃ´ng liÃªn quan ESG |

### Washing Types (7 classes)

| Type | Description |
|------|-------------|
| `NOT_WASHING` | Cam káº¿t genuine, cÃ³ báº±ng chá»©ng rÃµ rÃ ng |
| `VAGUE_COMMITMENT` | Cam káº¿t mÆ¡ há»“, khÃ´ng cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ |
| `SELECTIVE_DISCLOSURE` | Chá»‰ nÃªu Ä‘iá»ƒm tá»‘t, giáº¥u Ä‘iá»ƒm xáº¥u |
| `SYMBOLIC_ACTION` | HÃ nh Ä‘á»™ng mang tÃ­nh biá»ƒu tÆ°á»£ng |
| `DECOUPLING` | NÃ³i má»™t Ä‘áº±ng lÃ m má»™t náº»o |
| `MISLEADING_METRICS` | Sá»‘ liá»‡u gÃ¢y hiá»ƒu láº§m |
| `FUTURE_DEFLECTION` | TrÃ¬ hoÃ£n sang tÆ°Æ¡ng lai |

---

## ğŸ”§ Configuration

### Environment Variables (.env)

```env
# ==============================================================================
# LLM PROVIDER SELECTION
# Options: "gemini", "bedrock", "qwen"
# ==============================================================================
LLM_PROVIDER=gemini

# ==============================================================================
# GOOGLE GEMINI (recommended)
# ==============================================================================
GOOGLE_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-2.5-flash-lite

# ==============================================================================
# AWS BEDROCK (alternative)
# ==============================================================================
AWS_BEDROCK_REGION=us-east-1
BEDROCK_MODEL=claude-3.7-sonnet
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# ==============================================================================
# QWEN3 (self-hosted)
# ==============================================================================
QWEN_BASE_URL=http://your-server:8000/v1/chat/completions
QWEN_AUTH_USERNAME=username
QWEN_AUTH_PASSWORD=password
```

---

## ğŸ”„ Complete Workflow

```bash
# 1. Process raw OCR â†’ semantic chunks
python main.py process -i data/raw_ocr_annual_report.zip -o data/all_chunks.csv

# 2. Generate labels with LLM (multi-threaded)
python main.py label -i data/all_chunks.csv -o data/labeled.csv -w 4

# 3. Train ESG classifier
python main.py train --model-type esg --input data/labeled.csv --epochs 5

# 4. Train Washing detector
python main.py train --model-type washing --input data/labeled.csv --epochs 10

# 5. Classify new documents
python main.py classify -i new_data.csv -o classified.csv

# 6. Analyze for washing
python main.py analyze -i classified.csv --bank bidv --year 2024
```

---

## ğŸ Python API

### ESG Classification

```python
from models import HuggingFaceESGClassifierInference

# Load pre-trained model
classifier = HuggingFaceESGClassifierInference()

# Predict
result = classifier.predict("NgÃ¢n hÃ ng cam káº¿t giáº£m phÃ¡t tháº£i carbon")
print(f"Label: {result.predicted_label}, Confidence: {result.confidence:.2%}")

# Batch prediction
results = classifier.predict_batch(["Text 1", "Text 2", "Text 3"])
```

### Document Analysis

```python
from claim_evidence import DocumentAnalyzer

analyzer = DocumentAnalyzer(device="cuda")
result = analyzer.analyze_document(sentences, bank="agribank", year=2024)

print(f"Washing Index: {result.document_washing_index:.3f}")
print(f"High Risk Claims: {result.high_risk_claims}")
```

### LLM Clients

```python
from core import GeminiClient, BedrockClient

# Gemini
client = GeminiClient()
result = client.generate_content("Classify this text...")

# AWS Bedrock
client = BedrockClient(region="us-east-1", model_id="claude-3.7-sonnet")
result = client.generate_content("Classify this text...")
```

---

## ğŸ“š Pre-trained Models

| Model | HuggingFace Hub | Description |
|-------|-----------------|-------------|
| ESG Classifier | `huypham71/esgify_vn_class_weights` | 6-class ESG topic classifier |

---

## ğŸ“– References

- **PhoBERT**: Nguyen & Tuan Nguyen (2020). Pre-trained language models for Vietnamese.
- **ESGBERT**: Schimanski et al. (2024). ClimateBERT-based ESG classification.
- **A3CG Dataset**: Ong et al. (2025). Asian Anti-Greenwashing Claim-Context dataset.

---

## ğŸ‘¤ Author

**Huy Pham**  
University of Engineering and Technology (UET), Vietnam National University

---

## ğŸ“„ License

This project is for academic research purposes.
