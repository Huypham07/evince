# evince

<p align="center">
  <b>Evidence-Verified INtegrity Checker for ESG Claims</b><br>
  <i>A Novel Framework for ESG-Washing Detection in Vietnamese Banking Reports</i>
</p>

---

## ğŸ¯ Overview

**evince** lÃ  má»™t framework Deep Learning Ä‘á»ƒ phÃ¡t hiá»‡n **ESG-Washing** (táº©y xanh) trong bÃ¡o cÃ¡o thÆ°á»ng niÃªn cá»§a cÃ¡c ngÃ¢n hÃ ng Viá»‡t Nam. Framework sá»­ dá»¥ng **PhoBERT** lÃ m encoder ngÃ´n ngá»¯ vÃ  tÃ­ch há»£p **Claim-Evidence Linking** Ä‘á»ƒ phÃ¢n tÃ­ch á»Ÿ má»©c Ä‘á»™ document.

### TÃ­nh nÄƒng chÃ­nh

| Feature | MÃ´ táº£ |
|---------|-------|
| ğŸ·ï¸ **ESG Classification** | PhÃ¢n loáº¡i cÃ¢u vÄƒn vÃ o 6 chá»§ Ä‘á» ESG (max 512 tokens) |
| ğŸ” **Washing Detection** | PhÃ¡t hiá»‡n 7 loáº¡i ESG-Washing vá»›i attention explainability |
| ğŸ“„ **Document Analysis** | PhÃ¢n tÃ­ch má»©c Ä‘á»™ washing toÃ n bá»™ document |
| ğŸ”— **Claim-Evidence Linking** | LiÃªn káº¿t cam káº¿t vá»›i báº±ng chá»©ng há»— trá»£ |
| ğŸ“ **Semantic Chunking** | Xá»­ lÃ½ raw OCR thÃ nh semantic chunks vá»›i token limit |
| ğŸ¤– **LLM Labeling** | Táº¡o nhÃ£n tá»± Ä‘á»™ng vá»›i Qwen3 14B |

---


## ğŸ—ï¸ Project Structure

```
evince/
â”œâ”€â”€ main.py                 # ğŸš€ CLI entry point
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ metrics_visualizer.py   # Metrics plotting
â”‚
â”œâ”€â”€ data/                   # ğŸ“Š Data directory
â”‚   â”œâ”€â”€ raw_ocr_annual_report.zip  # Raw OCR text files
â”‚   â””â”€â”€ semantic_chunks.csv        # Processed chunks
â”‚
â”œâ”€â”€ models/                 # ğŸ§  Classification models
â”‚   â”œâ”€â”€ esg_topic_classifier.py    # ESG 6-class classifier (512 tokens)
â”‚   â””â”€â”€ washing_detector.py        # Washing 7-class detector
â”œâ”€â”€ claim_evidence/         # ğŸ”— Claim-Evidence Linking
â”œâ”€â”€ training/               # ğŸ‹ï¸ Training pipeline
â”‚   â”œâ”€â”€ train.py            # Trainer class
â”‚   â””â”€â”€ data_loader.py      # Dataset & DataLoader (512 tokens)
â”œâ”€â”€ evaluation/             # ğŸ“ˆ Metrics
â””â”€â”€ scripts/                # ğŸ“œ Utility scripts
    â”œâ”€â”€ llm_labeling.py     # LLM-based pseudo-labeling
    â””â”€â”€ process_ocr_semantic.py  # Smart OCR processing
```

---

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Clone repo
git clone https://github.com/Huypham07/evince.git
cd evince

# Install dependencies
pip install torch transformers pandas tqdm python-dotenv requests scikit-learn

# Setup env
cp .env.example .env
```

### 2. Process Raw OCR Data â†’ Semantic Chunks

Náº¿u báº¡n cÃ³ file raw OCR (txt/zip), sá»­ dá»¥ng **semantic chunking** Ä‘á»ƒ chia thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ nghÄ©a:

```bash
# Xá»­ lÃ½ file Ä‘Æ¡n
python main.py process --input data/bctn_2024_raw.txt --output data/chunks.csv

# Xá»­ lÃ½ zip chá»©a nhiá»u file
python main.py process --input data/raw_ocr_annual_report.zip --output data/all_chunks.csv
```

**Output CSV sáº½ cÃ³ cÃ¡c cá»™t:**
- `text`: Ná»™i dung chunk (Ä‘áº£m báº£o â‰¤500 tokens)
- `section`: TÃªn section (tá»« markdown headers `##`)
- `chunk_type`: `paragraph` hoáº·c `table`
- `bank`, `year`, `report_type`: Metadata tá»« filename
- `token_count`: Sá»‘ token thá»±c táº¿ (Ä‘áº¿m báº±ng PhoBERT tokenizer)

> ğŸ’¡ **TÃ­nh nÄƒng**: Script sá»­ dá»¥ng PhoBERT tokenizer Ä‘á»ƒ Ä‘áº¿m token chÃ­nh xÃ¡c vÃ  tá»± Ä‘á»™ng chia chunk náº¿u vÆ°á»£t 500 tokens.

### 3. Classify ESG Topics

```bash
# Classify tá»« file chunks
python main.py classify --input data/chunks.csv --output data/classified.csv

# Classify single text
python main.py classify --text "NgÃ¢n hÃ ng cam káº¿t giáº£m 30% phÃ¡t tháº£i carbon vÃ o nÄƒm 2030"
```

### 4. Generate Labels with LLM (Optional)

Náº¿u chÆ°a cÃ³ dá»¯ liá»‡u gÃ¡n nhÃ£n, sá»­ dá»¥ng LLM Ä‘á»ƒ táº¡o nhÃ£n tá»± Ä‘á»™ng:

```bash
# Cáº¥u hÃ¬nh Qwen/Gemini trong .env trÆ°á»›c
python main.py label --input data/chunks.csv --output data/labeled.csv --sample 2000
```

### 5. Train Model ğŸ‹ï¸

Báº¡n cÃ³ thá»ƒ train láº¡i model trÃªn dá»¯ liá»‡u cá»§a mÃ¬nh:

**Train ESG Topic Classifier:**
```bash
python main.py train \
    --model-type esg \
    --input data/labeled.csv \
    --epochs 5 \
    --output-dir ./checkpoints/esg
```

**Train Washing Detector:**
```bash
python main.py train \
    --model-type washing \
    --input data/labeled.csv \
    --epochs 10 \
    --output-dir ./checkpoints/washing
```

> ğŸ“ **Note**: Models máº·c Ä‘á»‹nh sá»­ dá»¥ng `max_length=512` vÃ  `freeze_bert_layers=0` (full fine-tuning) Ä‘á»ƒ hiá»ƒu tá»‘t ngá»¯ cáº£nh Ä‘oáº¡n vÄƒn.

### 6. Document Analysis (Detection) ğŸ”

PhÃ¢n tÃ­ch tÃ i liá»‡u Ä‘á»ƒ tÃ¬m ESG-washing vÃ  **xem báº±ng chá»©ng cá»¥ thá»ƒ**:

```bash
python main.py analyze --input data/classified.csv --bank agribank --year 2024 --verbose
```

**Output máº«u:**
```
============================================================
DOCUMENT ANALYSIS RESULT
============================================================
Bank: agribank | Year: 2024
Document Washing Index: 0.412
High Risk Claims: 5
...
âš ï¸  HIGH RISK CLAIMS DETECTED (Washing Evidence):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[1] Claim: "NgÃ¢n hÃ ng cam káº¿t Ä‘áº¡t Net Zero vÃ o nÄƒm 2050"
    Risk Level: HIGH
    Verification Score: 0.120
    Evidence Found:
      (No relevant evidence found)

[2] Claim: "ChÃºng tÃ´i luÃ´n há»— trá»£ cá»™ng Ä‘á»“ng bá»‹ áº£nh hÆ°á»Ÿng thiÃªn tai"
    Risk Level: MEDIUM
    Verification Score: 0.450
    Evidence Found:
      - [0.48] NgÃ¢n hÃ ng Ä‘Ã£ quyÃªn gÃ³p 5 tá»· Ä‘á»“ng cho quá»¹ cá»©u trá»£ miá»n Trung.
```

---

## ğŸ“Š ESG Labels

### ESG Topic Classification (6 classes)

| Label | Description | Vietnamese |
|-------|-------------|------------|
| `Environmental_Performance` | MÃ´i trÆ°á»ng, khÃ­ háº­u, nÄƒng lÆ°á»£ng | Hiá»‡u quáº£ mÃ´i trÆ°á»ng |
| `Social_Performance` | NhÃ¢n viÃªn, cá»™ng Ä‘á»“ng, xÃ£ há»™i | Hiá»‡u quáº£ xÃ£ há»™i |
| `Governance_Performance` | Quáº£n trá»‹, Ä‘áº¡o Ä‘á»©c, tuÃ¢n thá»§ | Hiá»‡u quáº£ quáº£n trá»‹ |
| `ESG_Financing` | TÃ­n dá»¥ng xanh, trÃ¡i phiáº¿u ESG | TÃ i chÃ­nh ESG |
| `Strategy_and_Policy` | Chiáº¿n lÆ°á»£c, chÃ­nh sÃ¡ch ESG | Chiáº¿n lÆ°á»£c & ChÃ­nh sÃ¡ch |
| `Not_ESG_Related` | KhÃ´ng liÃªn quan ESG | KhÃ´ng liÃªn quan |

### Washing Types (7 classes)

| Type | Description |
|------|-------------|
| `NOT_WASHING` | Cam káº¿t genuine, cÃ³ báº±ng chá»©ng rÃµ rÃ ng |
| `VAGUE_COMMITMENT` | Cam káº¿t mÆ¡ há»“, khÃ´ng cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ |
| `SELECTIVE_DISCLOSURE` | Chá»‰ nÃªu Ä‘iá»ƒm tá»‘t, giáº¥u Ä‘iá»ƒm xáº¥u |
| `SYMBOLIC_ACTION` | HÃ nh Ä‘á»™ng mang tÃ­nh biá»ƒu tÆ°á»£ng |
| `DECOUPLING` | NÃ³i má»™t Ä‘áº±ng lÃ m má»™t náº»o |
| `MISLEADING_METRICS` | Sá»‘ liá»‡u gÃ¢y hiá»ƒu láº§m |
| `FUTURE_DEFLECTION` | TrÃ¬ hoÃ£n sang tÆ°Æ¡ng lai |

---

## ğŸ Python API

### ESG Classification

```python
from evince.models import HuggingFaceESGClassifierInference

# Load pre-trained model from HuggingFace
classifier = HuggingFaceESGClassifierInference()

# Single prediction (supports up to 512 tokens)
result = classifier.predict("NgÃ¢n hÃ ng cam káº¿t giáº£m phÃ¡t tháº£i carbon 30% vÃ o nÄƒm 2030")
print(f"Label: {result.predicted_label}")
print(f"Confidence: {result.confidence:.2%}")

# Batch prediction
results = classifier.predict_batch(["Äoáº¡n vÄƒn 1", "Äoáº¡n vÄƒn 2", "Äoáº¡n vÄƒn 3"])
```

### Document Analysis

```python
from evince.claim_evidence import DocumentAnalyzer

analyzer = DocumentAnalyzer(device="cuda")

result = analyzer.analyze_document(
    sentences=["Cam káº¿t 1", "Báº±ng chá»©ng 1", "Cam káº¿t 2"],
    bank="agribank",
    year=2024
)

print(f"Washing Index: {result.document_washing_index:.3f}")
print(f"High Risk Claims: {result.high_risk_claims}")
```

### Process Raw OCR

```python
from evince.scripts.process_ocr_semantic import process_single_file, chunks_to_csv

# Process raw OCR file
chunks = process_single_file("data/bctn_2024_raw.txt")

# Save to CSV
chunks_to_csv(chunks, "data/chunks.csv")

# Each chunk has:
# - text (â‰¤500 tokens)
# - section, chunk_type
# - bank, year, report_type
# - token_count
```

---

## ğŸ”§ Configuration

### Environment Variables (.env)

```env
# Qwen3 LLM (for pseudo-labeling)
QWEN_BASE_URL=http://your-server:8000/v1/chat/completions
QWEN_AUTH_USERNAME=your_username
QWEN_AUTH_PASSWORD=your_password
QWEN_MODEL=Qwen3-14B

# Optional: Google Gemini
GOOGLE_API_KEY=your_api_key
```

---

## ğŸ“š Pre-trained Models

| Model | HuggingFace Hub | Max Tokens | Description |
|-------|-----------------|------------|-------------|
| ESG Classifier | `huypham71/esgify_vn_class_weights` | 512 | 6-class ESG topic classifier |

---

## ğŸ”„ Complete Workflow

```bash
# 1. Process raw OCR â†’ semantic chunks (with token limit)
python main.py process -i data/bctn_2024_raw.txt -o data/chunks.csv

# 2. Classify ESG topics
python main.py classify -i data/chunks.csv -o data/classified.csv

# 3. (Optional) Generate labels for training
python main.py label -i data/chunks.csv -o data/labeled.csv --sample 500

# 4. (Optional) Train custom model
python main.py train --model-type esg --input data/labeled.csv --epochs 5

# 5. Analyze for washing detection
python main.py analyze -i data/classified.csv --bank agribank --year 2024
```

---

## ğŸ“– References

- **PhoBERT**: Nguyen & Tuan Nguyen (2020). PhoBERT: Pre-trained language models for Vietnamese.
- **ESGBERT**: Schimanski et al. (2024). ClimateBERT-based ESG classification.
- **A3CG Dataset**: Ong et al. (2025). Asian Anti-Greenwashing Claim-Context dataset.

---

## ğŸ‘¤ Author

**Huy Pham**  
University of Engineering and Technology (UET), Vietnam National University

---

## ğŸ“„ License

This project is for academic research purposes.
