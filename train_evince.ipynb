{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EVINCE: Complete Training Pipeline (Self-Contained)\n",
                "\n",
                "**Vietnamese Banking ESG-Washing Detection**\n",
                "\n",
                "This notebook is **fully self-contained** - all code is included.\n",
                "\n",
                "**Models:**\n",
                "1. **ESG Topic Classifier** - PhoBERT + Classification Head\n",
                "2. **Washing Detector** - PhoBERT + Multi-task Head"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q torch transformers pandas scikit-learn tqdm huggingface_hub matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from typing import Dict, List, Optional, Tuple\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.optim import AdamW\n",
                "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'data_path': 'data/labeled.csv',\n",
                "    'model_name': 'vinai/phobert-base-v2',\n",
                "    'max_length': 256,  # PhoBERT max = 258\n",
                "    'batch_size': 16,\n",
                "    'epochs': 5,\n",
                "    'learning_rate': 2e-5,\n",
                "    'weight_decay': 0.01,\n",
                "    'warmup_ratio': 0.1,\n",
                "    'undersample_ratio': 1.0,\n",
                "    'output_dir': './checkpoints',\n",
                "    'hf_username': 'huypham71',\n",
                "}\n",
                "\n",
                "# ESG Labels\n",
                "ESG_LABELS = [\n",
                "    \"Environmental_Performance\",\n",
                "    \"Social_Performance\",\n",
                "    \"Governance_Performance\",\n",
                "    \"ESG_Financing\",\n",
                "    \"Strategy_and_Policy\",\n",
                "    \"Not_ESG_Related\"\n",
                "]\n",
                "ESG_LABEL_TO_ID = {label: i for i, label in enumerate(ESG_LABELS)}\n",
                "ESG_ID_TO_LABEL = {i: label for i, label in enumerate(ESG_LABELS)}\n",
                "\n",
                "SHORT_TO_FULL = {\n",
                "    'E': 'Environmental_Performance',\n",
                "    'S': 'Social_Performance',\n",
                "    'G': 'Governance_Performance',\n",
                "    'Financing': 'ESG_Financing',\n",
                "    'Policy': 'Strategy_and_Policy',\n",
                "    'Non-ESG': 'Not_ESG_Related',\n",
                "}\n",
                "\n",
                "# Washing Labels\n",
                "WASHING_LABELS = [\n",
                "    \"NO_WASHING\",\n",
                "    \"CHERRY_PICKING\",\n",
                "    \"VAGUE_CLAIMS\",\n",
                "    \"LACK_OF_PROOF\",\n",
                "    \"MISLEADING_IMAGERY\",\n",
                "    \"FALSE_LABELS\",\n",
                "    \"SELECTIVE_DISCLOSURE\"\n",
                "]\n",
                "WASHING_LABEL_TO_ID = {label: i for i, label in enumerate(WASHING_LABELS)}\n",
                "WASHING_ID_TO_LABEL = {i: label for i, label in enumerate(WASHING_LABELS)}\n",
                "\n",
                "print(\"Configuration loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Architectures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ESGTopicClassifier(nn.Module):\n",
                "    \"\"\"\n",
                "    ESG Topic Classifier using PhoBERT.\n",
                "    \n",
                "    Architecture:\n",
                "    - PhoBERT encoder (vinai/phobert-base-v2)\n",
                "    - Dropout (0.3)\n",
                "    - Linear (768 -> 256)\n",
                "    - ReLU\n",
                "    - Dropout (0.2)\n",
                "    - Linear (256 -> num_classes)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        model_name: str = \"vinai/phobert-base-v2\",\n",
                "        num_classes: int = 6,\n",
                "        dropout: float = 0.3,\n",
                "        freeze_bert_layers: int = 0\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.num_classes = num_classes\n",
                "        \n",
                "        # Load PhoBERT encoder\n",
                "        self.encoder = AutoModel.from_pretrained(model_name)\n",
                "        self.hidden_size = self.encoder.config.hidden_size  # 768\n",
                "        \n",
                "        # Freeze layers if specified\n",
                "        if freeze_bert_layers > 0:\n",
                "            for param in self.encoder.embeddings.parameters():\n",
                "                param.requires_grad = False\n",
                "            for i, layer in enumerate(self.encoder.encoder.layer):\n",
                "                if i < freeze_bert_layers:\n",
                "                    for param in layer.parameters():\n",
                "                        param.requires_grad = False\n",
                "        \n",
                "        # Classification head\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(self.hidden_size, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.2),\n",
                "            nn.Linear(256, num_classes)\n",
                "        )\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids: torch.Tensor,\n",
                "        attention_mask: torch.Tensor\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Args:\n",
                "            input_ids: Token IDs [batch_size, seq_len]\n",
                "            attention_mask: Attention mask [batch_size, seq_len]\n",
                "        \n",
                "        Returns:\n",
                "            logits: [batch_size, num_classes]\n",
                "        \"\"\"\n",
                "        # Create token_type_ids (required for RoBERTa)\n",
                "        token_type_ids = torch.zeros_like(input_ids)\n",
                "        \n",
                "        # Encode with PhoBERT\n",
                "        outputs = self.encoder(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids\n",
                "        )\n",
                "        \n",
                "        # Use [CLS] token representation\n",
                "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
                "        \n",
                "        # Classify\n",
                "        logits = self.classifier(cls_output)\n",
                "        \n",
                "        return logits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class WashingDetector(nn.Module):\n",
                "    \"\"\"\n",
                "    Washing Detector with multi-task learning.\n",
                "    \n",
                "    Architecture:\n",
                "    - PhoBERT encoder\n",
                "    - Shared hidden layer\n",
                "    - Two heads:\n",
                "      1. Washing type classifier (7 classes)\n",
                "      2. Confidence regressor\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        model_name: str = \"vinai/phobert-base-v2\",\n",
                "        num_classes: int = 7,\n",
                "        dropout: float = 0.3,\n",
                "        freeze_bert_layers: int = 0\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.num_classes = num_classes\n",
                "        \n",
                "        # Load PhoBERT encoder\n",
                "        self.encoder = AutoModel.from_pretrained(model_name)\n",
                "        self.hidden_size = self.encoder.config.hidden_size\n",
                "        \n",
                "        # Freeze layers if specified\n",
                "        if freeze_bert_layers > 0:\n",
                "            for param in self.encoder.embeddings.parameters():\n",
                "                param.requires_grad = False\n",
                "            for i, layer in enumerate(self.encoder.encoder.layer):\n",
                "                if i < freeze_bert_layers:\n",
                "                    for param in layer.parameters():\n",
                "                        param.requires_grad = False\n",
                "        \n",
                "        # Shared hidden layer\n",
                "        self.shared = nn.Sequential(\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(self.hidden_size, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.2)\n",
                "        )\n",
                "        \n",
                "        # Washing type classifier\n",
                "        self.type_classifier = nn.Linear(256, num_classes)\n",
                "        \n",
                "        # Confidence regressor (0-1)\n",
                "        self.confidence_regressor = nn.Sequential(\n",
                "            nn.Linear(256, 1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids: torch.Tensor,\n",
                "        attention_mask: torch.Tensor\n",
                "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Returns:\n",
                "            type_logits: [batch_size, num_classes]\n",
                "            confidence: [batch_size, 1]\n",
                "        \"\"\"\n",
                "        token_type_ids = torch.zeros_like(input_ids)\n",
                "        \n",
                "        outputs = self.encoder(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            token_type_ids=token_type_ids\n",
                "        )\n",
                "        \n",
                "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
                "        shared_output = self.shared(cls_output)\n",
                "        \n",
                "        type_logits = self.type_classifier(shared_output)\n",
                "        confidence = self.confidence_regressor(shared_output)\n",
                "        \n",
                "        return type_logits, confidence"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ESGDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset for ESG/Washing classification.\n",
                "    \n",
                "    Handles:\n",
                "    - Tokenization with PhoBERT tokenizer\n",
                "    - Padding/truncation\n",
                "    - Token ID validation (clamp to vocab range)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        sentences: List[str],\n",
                "        labels: List[int],\n",
                "        tokenizer_name: str = \"vinai/phobert-base-v2\",\n",
                "        max_length: int = 256\n",
                "    ):\n",
                "        self.sentences = sentences\n",
                "        self.labels = labels\n",
                "        self.max_length = max_length\n",
                "        \n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
                "        self.vocab_size = self.tokenizer.vocab_size\n",
                "        \n",
                "        print(f\"Dataset: {len(sentences)} samples, vocab={self.vocab_size}, max_len={max_length}\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.sentences)\n",
                "    \n",
                "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
                "        sentence = self.sentences[idx]\n",
                "        label = self.labels[idx]\n",
                "        \n",
                "        # Handle None/invalid sentences\n",
                "        if sentence is None or not isinstance(sentence, str):\n",
                "            sentence = \"\"\n",
                "        sentence = str(sentence).strip()\n",
                "        if not sentence:\n",
                "            sentence = \"[UNK]\"\n",
                "        \n",
                "        # Tokenize\n",
                "        encoding = self.tokenizer(\n",
                "            sentence,\n",
                "            max_length=self.max_length,\n",
                "            padding=\"max_length\",\n",
                "            truncation=True,\n",
                "            return_tensors=\"pt\"\n",
                "        )\n",
                "        \n",
                "        # Clamp token IDs to valid range\n",
                "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
                "        input_ids = torch.clamp(input_ids, min=0, max=self.vocab_size - 1)\n",
                "        \n",
                "        return {\n",
                "            \"input_ids\": input_ids,\n",
                "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
                "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Trainer Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Trainer:\n",
                "    \"\"\"\n",
                "    Training loop with:\n",
                "    - Weighted CrossEntropyLoss (for imbalanced data)\n",
                "    - AdamW optimizer with weight decay\n",
                "    - Linear warmup scheduler\n",
                "    - Checkpoint saving\n",
                "    - Validation evaluation\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        model: nn.Module,\n",
                "        train_loader: DataLoader,\n",
                "        val_loader: DataLoader,\n",
                "        learning_rate: float = 2e-5,\n",
                "        weight_decay: float = 0.01,\n",
                "        num_epochs: int = 5,\n",
                "        warmup_ratio: float = 0.1,\n",
                "        output_dir: str = \"./checkpoints\",\n",
                "        device: str = \"cuda\",\n",
                "        class_weights: torch.Tensor = None\n",
                "    ):\n",
                "        self.device = torch.device(device)\n",
                "        self.model = model.to(self.device)\n",
                "        self.train_loader = train_loader\n",
                "        self.val_loader = val_loader\n",
                "        self.num_epochs = num_epochs\n",
                "        self.output_dir = output_dir\n",
                "        os.makedirs(output_dir, exist_ok=True)\n",
                "        \n",
                "        # Optimizer\n",
                "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
                "        optimizer_params = [\n",
                "            {\"params\": [p for n, p in model.named_parameters() \n",
                "                       if not any(nd in n for nd in no_decay)],\n",
                "             \"weight_decay\": weight_decay},\n",
                "            {\"params\": [p for n, p in model.named_parameters() \n",
                "                       if any(nd in n for nd in no_decay)],\n",
                "             \"weight_decay\": 0.0},\n",
                "        ]\n",
                "        self.optimizer = AdamW(optimizer_params, lr=learning_rate)\n",
                "        \n",
                "        # Scheduler\n",
                "        total_steps = len(train_loader) * num_epochs\n",
                "        warmup_steps = int(total_steps * warmup_ratio)\n",
                "        self.scheduler = get_linear_schedule_with_warmup(\n",
                "            self.optimizer, warmup_steps, total_steps\n",
                "        )\n",
                "        \n",
                "        # Loss function\n",
                "        if class_weights is not None:\n",
                "            class_weights = class_weights.to(self.device)\n",
                "            self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
                "            print(\"Using weighted CrossEntropyLoss\")\n",
                "        else:\n",
                "            self.criterion = nn.CrossEntropyLoss()\n",
                "        \n",
                "        self.best_val_acc = 0.0\n",
                "        self.history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
                "    \n",
                "    def train_epoch(self, epoch: int) -> float:\n",
                "        self.model.train()\n",
                "        total_loss = 0.0\n",
                "        \n",
                "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
                "        for batch in pbar:\n",
                "            input_ids = batch[\"input_ids\"].to(self.device)\n",
                "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
                "            labels = batch[\"labels\"].to(self.device)\n",
                "            \n",
                "            self.optimizer.zero_grad()\n",
                "            \n",
                "            outputs = self.model(input_ids, attention_mask)\n",
                "            \n",
                "            # Handle multi-task output\n",
                "            if isinstance(outputs, tuple):\n",
                "                logits = outputs[0]\n",
                "            else:\n",
                "                logits = outputs\n",
                "            \n",
                "            loss = self.criterion(logits, labels)\n",
                "            loss.backward()\n",
                "            \n",
                "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
                "            \n",
                "            self.optimizer.step()\n",
                "            self.scheduler.step()\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
                "        \n",
                "        return total_loss / len(self.train_loader)\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def evaluate(self) -> Dict[str, float]:\n",
                "        self.model.eval()\n",
                "        total_loss = 0.0\n",
                "        correct = 0\n",
                "        total = 0\n",
                "        \n",
                "        for batch in self.val_loader:\n",
                "            input_ids = batch[\"input_ids\"].to(self.device)\n",
                "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
                "            labels = batch[\"labels\"].to(self.device)\n",
                "            \n",
                "            outputs = self.model(input_ids, attention_mask)\n",
                "            if isinstance(outputs, tuple):\n",
                "                logits = outputs[0]\n",
                "            else:\n",
                "                logits = outputs\n",
                "            \n",
                "            loss = self.criterion(logits, labels)\n",
                "            total_loss += loss.item()\n",
                "            \n",
                "            preds = logits.argmax(dim=-1)\n",
                "            correct += (preds == labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "        \n",
                "        return {\n",
                "            \"val_loss\": total_loss / len(self.val_loader),\n",
                "            \"val_acc\": correct / total\n",
                "        }\n",
                "    \n",
                "    @torch.no_grad()\n",
                "    def evaluate_loader(self, data_loader: DataLoader) -> Dict[str, float]:\n",
                "        \"\"\"Evaluate on any DataLoader (for test set).\"\"\"\n",
                "        self.model.eval()\n",
                "        total_loss = 0.0\n",
                "        all_preds = []\n",
                "        all_labels = []\n",
                "        \n",
                "        for batch in data_loader:\n",
                "            input_ids = batch[\"input_ids\"].to(self.device)\n",
                "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
                "            labels = batch[\"labels\"].to(self.device)\n",
                "            \n",
                "            outputs = self.model(input_ids, attention_mask)\n",
                "            if isinstance(outputs, tuple):\n",
                "                logits = outputs[0]\n",
                "            else:\n",
                "                logits = outputs\n",
                "            \n",
                "            loss = self.criterion(logits, labels)\n",
                "            total_loss += loss.item()\n",
                "            \n",
                "            preds = logits.argmax(dim=-1)\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "        \n",
                "        return {\n",
                "            \"loss\": total_loss / len(data_loader),\n",
                "            \"accuracy\": accuracy_score(all_labels, all_preds),\n",
                "            \"f1_macro\": f1_score(all_labels, all_preds, average='macro'),\n",
                "            \"f1_weighted\": f1_score(all_labels, all_preds, average='weighted')\n",
                "        }\n",
                "    \n",
                "    def save_checkpoint(self, epoch: int, is_best: bool = False):\n",
                "        checkpoint = {\n",
                "            \"epoch\": epoch,\n",
                "            \"model_state_dict\": self.model.state_dict(),\n",
                "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
                "            \"best_val_acc\": self.best_val_acc,\n",
                "        }\n",
                "        torch.save(checkpoint, f\"{self.output_dir}/checkpoint_latest.pt\")\n",
                "        if is_best:\n",
                "            torch.save(checkpoint, f\"{self.output_dir}/checkpoint_best.pt\")\n",
                "    \n",
                "    def train(self):\n",
                "        print(f\"Training on {self.device}\")\n",
                "        print(f\"Epochs: {self.num_epochs}, Batch size: {self.train_loader.batch_size}\")\n",
                "        \n",
                "        for epoch in range(self.num_epochs):\n",
                "            train_loss = self.train_epoch(epoch)\n",
                "            val_metrics = self.evaluate()\n",
                "            \n",
                "            self.history[\"train_loss\"].append(train_loss)\n",
                "            self.history[\"val_loss\"].append(val_metrics[\"val_loss\"])\n",
                "            self.history[\"val_acc\"].append(val_metrics[\"val_acc\"])\n",
                "            \n",
                "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
                "            print(f\"  Val Loss: {val_metrics['val_loss']:.4f}, Val Acc: {val_metrics['val_acc']:.4f}\")\n",
                "            \n",
                "            is_best = val_metrics[\"val_acc\"] > self.best_val_acc\n",
                "            if is_best:\n",
                "                self.best_val_acc = val_metrics[\"val_acc\"]\n",
                "                print(f\"  New best! Saving checkpoint...\")\n",
                "            \n",
                "            self.save_checkpoint(epoch, is_best)\n",
                "        \n",
                "        print(f\"\\nTraining complete. Best Val Acc: {self.best_val_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload data/labeled.csv or use your path\n",
                "df = pd.read_csv(CONFIG['data_path'])\n",
                "text_col = 'text' if 'text' in df.columns else 'sentence'\n",
                "\n",
                "print(f\"Dataset: {len(df)} samples\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df['esg_label'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part A: Train ESG Classifier\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare ESG data with balancing\n",
                "df_esg = df[df['esg_label'].notna()].copy()\n",
                "df_esg['label_mapped'] = df_esg['esg_label'].map(lambda x: SHORT_TO_FULL.get(x, x))\n",
                "df_esg = df_esg[df_esg['label_mapped'].isin(ESG_LABELS)].reset_index(drop=True)\n",
                "\n",
                "# Undersample Non-ESG\n",
                "label_counts = df_esg['esg_label'].value_counts()\n",
                "second_largest = label_counts.iloc[1]\n",
                "max_non_esg = int(second_largest * CONFIG['undersample_ratio'])\n",
                "\n",
                "df_non_esg = df_esg[df_esg['esg_label'] == 'Non-ESG']\n",
                "df_other = df_esg[df_esg['esg_label'] != 'Non-ESG']\n",
                "\n",
                "if len(df_non_esg) > max_non_esg:\n",
                "    df_non_esg = df_non_esg.sample(n=max_non_esg, random_state=42)\n",
                "\n",
                "df_esg_balanced = pd.concat([df_other, df_non_esg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(f\"Balanced: {len(df_esg_balanced)} samples\")\n",
                "print(df_esg_balanced['esg_label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class weights\n",
                "label_counts_bal = df_esg_balanced['label_mapped'].value_counts()\n",
                "total = len(df_esg_balanced)\n",
                "n_classes = len(ESG_LABELS)\n",
                "\n",
                "esg_class_weights = {}\n",
                "for label in ESG_LABELS:\n",
                "    count = label_counts_bal.get(label, 1)\n",
                "    esg_class_weights[ESG_LABEL_TO_ID[label]] = total / (n_classes * count)\n",
                "\n",
                "esg_weight_tensor = torch.tensor([esg_class_weights[i] for i in range(n_classes)], dtype=torch.float)\n",
                "print(\"Class weights:\", esg_class_weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified split\n",
                "texts = df_esg_balanced[text_col].tolist()\n",
                "labels = [ESG_LABEL_TO_ID[l] for l in df_esg_balanced['label_mapped']]\n",
                "\n",
                "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
                "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
                ")\n",
                "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
                "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
                ")\n",
                "\n",
                "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets\n",
                "esg_train_dataset = ESGDataset(train_texts, train_labels, max_length=CONFIG['max_length'])\n",
                "esg_val_dataset = ESGDataset(val_texts, val_labels, max_length=CONFIG['max_length'])\n",
                "esg_test_dataset = ESGDataset(test_texts, test_labels, max_length=CONFIG['max_length'])\n",
                "\n",
                "esg_train_loader = DataLoader(esg_train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
                "esg_val_loader = DataLoader(esg_val_dataset, batch_size=CONFIG['batch_size'])\n",
                "esg_test_loader = DataLoader(esg_test_dataset, batch_size=CONFIG['batch_size'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "esg_model = ESGTopicClassifier(num_classes=6)\n",
                "\n",
                "total_params = sum(p.numel() for p in esg_model.parameters())\n",
                "trainable = sum(p.numel() for p in esg_model.parameters() if p.requires_grad)\n",
                "print(f\"Total params: {total_params:,}\")\n",
                "print(f\"Trainable: {trainable:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "esg_trainer = Trainer(\n",
                "    model=esg_model,\n",
                "    train_loader=esg_train_loader,\n",
                "    val_loader=esg_val_loader,\n",
                "    learning_rate=CONFIG['learning_rate'],\n",
                "    num_epochs=CONFIG['epochs'],\n",
                "    output_dir=f\"{CONFIG['output_dir']}/esg\",\n",
                "    device=str(DEVICE),\n",
                "    class_weights=esg_weight_tensor\n",
                ")\n",
                "\n",
                "esg_trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "esg_test_metrics = esg_trainer.evaluate_loader(esg_test_loader)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ESG CLASSIFIER - TEST RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Accuracy: {esg_test_metrics['accuracy']:.4f}\")\n",
                "print(f\"F1 Macro: {esg_test_metrics['f1_macro']:.4f}\")\n",
                "print(f\"F1 Weighted: {esg_test_metrics['f1_weighted']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report\n",
                "esg_model.eval()\n",
                "all_preds, all_labels = [], []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch in esg_test_loader:\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        outputs = esg_model(input_ids, attention_mask)\n",
                "        preds = outputs.argmax(dim=-1).cpu().numpy()\n",
                "        all_preds.extend(preds)\n",
                "        all_labels.extend(batch['labels'].numpy())\n",
                "\n",
                "label_names = [l.replace('_Performance', '').replace('_Related', '') for l in ESG_LABELS]\n",
                "print(classification_report(all_labels, all_preds, target_names=label_names, digits=3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('True')\n",
                "plt.title('ESG Classifier - Confusion Matrix')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part B: Train Washing Detector (Optional)\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if washing data exists\n",
                "if 'washing_type' in df.columns:\n",
                "    df_washing = df[df['esg_label'] != 'Non-ESG'].copy()\n",
                "    df_washing = df_washing[df_washing['washing_type'].notna()]\n",
                "    df_washing = df_washing[df_washing['washing_type'].isin(WASHING_LABELS)].reset_index(drop=True)\n",
                "    \n",
                "    print(f\"Washing samples: {len(df_washing)}\")\n",
                "    TRAIN_WASHING = len(df_washing) >= 100\n",
                "else:\n",
                "    print(\"No washing_type column found\")\n",
                "    TRAIN_WASHING = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if TRAIN_WASHING:\n",
                "    # Prepare washing data\n",
                "    w_texts = df_washing[text_col].tolist()\n",
                "    w_labels = [WASHING_LABEL_TO_ID[l] for l in df_washing['washing_type']]\n",
                "    \n",
                "    w_train_texts, w_temp_texts, w_train_labels, w_temp_labels = train_test_split(\n",
                "        w_texts, w_labels, test_size=0.2, random_state=42, stratify=w_labels\n",
                "    )\n",
                "    w_val_texts, w_test_texts, w_val_labels, w_test_labels = train_test_split(\n",
                "        w_temp_texts, w_temp_labels, test_size=0.5, random_state=42, stratify=w_temp_labels\n",
                "    )\n",
                "    \n",
                "    # Create datasets\n",
                "    w_train_dataset = ESGDataset(w_train_texts, w_train_labels, max_length=CONFIG['max_length'])\n",
                "    w_val_dataset = ESGDataset(w_val_texts, w_val_labels, max_length=CONFIG['max_length'])\n",
                "    w_test_dataset = ESGDataset(w_test_texts, w_test_labels, max_length=CONFIG['max_length'])\n",
                "    \n",
                "    w_train_loader = DataLoader(w_train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
                "    w_val_loader = DataLoader(w_val_dataset, batch_size=CONFIG['batch_size'])\n",
                "    w_test_loader = DataLoader(w_test_dataset, batch_size=CONFIG['batch_size'])\n",
                "    \n",
                "    # Class weights\n",
                "    w_counts = df_washing['washing_type'].value_counts()\n",
                "    w_weights = {}\n",
                "    for label in WASHING_LABELS:\n",
                "        count = w_counts.get(label, 1)\n",
                "        w_weights[WASHING_LABEL_TO_ID[label]] = len(df_washing) / (len(WASHING_LABELS) * count)\n",
                "    w_weight_tensor = torch.tensor([w_weights[i] for i in range(len(WASHING_LABELS))], dtype=torch.float)\n",
                "    \n",
                "    # Train\n",
                "    washing_model = WashingDetector(num_classes=7)\n",
                "    \n",
                "    washing_trainer = Trainer(\n",
                "        model=washing_model,\n",
                "        train_loader=w_train_loader,\n",
                "        val_loader=w_val_loader,\n",
                "        learning_rate=CONFIG['learning_rate'],\n",
                "        num_epochs=CONFIG['epochs'],\n",
                "        output_dir=f\"{CONFIG['output_dir']}/washing\",\n",
                "        device=str(DEVICE),\n",
                "        class_weights=w_weight_tensor\n",
                "    )\n",
                "    \n",
                "    washing_trainer.train()\n",
                "    \n",
                "    # Evaluate\n",
                "    w_test_metrics = washing_trainer.evaluate_loader(w_test_loader)\n",
                "    print(f\"\\nWashing Detector - Accuracy: {w_test_metrics['accuracy']:.4f}, F1: {w_test_metrics['f1_macro']:.4f}\")\n",
                "else:\n",
                "    print(\"Skipping Washing Detector training\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part C: Upload to HuggingFace\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import HfApi, login\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save and upload ESG model\n",
                "esg_save_path = f\"{CONFIG['output_dir']}/esg/hf\"\n",
                "os.makedirs(esg_save_path, exist_ok=True)\n",
                "\n",
                "torch.save({\n",
                "    'model_state_dict': esg_model.state_dict(),\n",
                "    'config': {'model_name': CONFIG['model_name'], 'num_classes': 6, 'max_length': CONFIG['max_length']},\n",
                "    'label_mapping': ESG_LABEL_TO_ID,\n",
                "    'test_metrics': esg_test_metrics\n",
                "}, f\"{esg_save_path}/pytorch_model.bin\")\n",
                "\n",
                "# Model card\n",
                "with open(f\"{esg_save_path}/README.md\", 'w') as f:\n",
                "    f.write(f\"\"\"---\n",
                "language: vi\n",
                "tags: [esg, text-classification, vietnamese, phobert]\n",
                "---\n",
                "# ESG Topic Classifier (Vietnamese)\n",
                "Accuracy: {esg_test_metrics['accuracy']:.4f}, F1 Macro: {esg_test_metrics['f1_macro']:.4f}\n",
                "\"\"\")\n",
                "\n",
                "api = HfApi()\n",
                "ESG_REPO = f\"{CONFIG['hf_username']}/evince-esg-classifier-v2\"\n",
                "api.create_repo(repo_id=ESG_REPO, exist_ok=True)\n",
                "api.upload_folder(folder_path=esg_save_path, repo_id=ESG_REPO)\n",
                "print(f\"Uploaded: https://huggingface.co/{ESG_REPO}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload Washing model if trained\n",
                "if TRAIN_WASHING:\n",
                "    w_save_path = f\"{CONFIG['output_dir']}/washing/hf\"\n",
                "    os.makedirs(w_save_path, exist_ok=True)\n",
                "    \n",
                "    torch.save({\n",
                "        'model_state_dict': washing_model.state_dict(),\n",
                "        'config': {'model_name': CONFIG['model_name'], 'num_classes': 7},\n",
                "        'label_mapping': WASHING_LABEL_TO_ID,\n",
                "        'test_metrics': w_test_metrics\n",
                "    }, f\"{w_save_path}/pytorch_model.bin\")\n",
                "    \n",
                "    WASHING_REPO = f\"{CONFIG['hf_username']}/evince-washing-detector-v2\"\n",
                "    api.create_repo(repo_id=WASHING_REPO, exist_ok=True)\n",
                "    api.upload_folder(folder_path=w_save_path, repo_id=WASHING_REPO)\n",
                "    print(f\"Uploaded: https://huggingface.co/{WASHING_REPO}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Done!\n",
                "\n",
                "Models uploaded to HuggingFace Hub."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}